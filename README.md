
# ğŸŒ± EcoMatch - Intelligent Product-to-Carbon Matching

EcoMatch is a semantic product matching system designed to map noisy product descriptions (e.g. from receipts) to structured ingredient data and return their carbon impact rating.

---

## ğŸš€ Pipeline Overview

### 1. **Preprocessing (One-time Setup)**

This step cleans, embeds, and indexes the ingredient reference dataset.

- Cleans product names using regex-based normalization
- Generates semantic embeddings using Sentence-BERT
- Saves:
  - `product_index.parquet`: Cleaned product dataset
  - `product_embeddings.npy`: NumPy array of embedding vectors
  - `faiss_index.bin`: Precomputed FAISS index for fast similarity search

Run:

```bash
python preprocess.py
```

---

### 2. **Matching Pipeline (For New Inputs)**

This runs the matching process for a given input list (`test.csv`) and produces `output.csv`.

For each product:
- Cleans the name
- Embeds the input using Sentence-BERT
- Searches top-K similar products using FAISS
- Re-ranks with a fuzzy-semantic scoring blend:  
  `score = Î± Ã— semantic + (1 - Î±) Ã— fuzzy_ratio`
- Returns: `input_product`, `matched_product`, `carbon_rating`

Run:

```bash
python run.py
```

The script logs:
- Matcher load time
- Input read time
- Per-product match latency
- Output write time
- Total runtime

---

### 3. **Evaluation**

To check how accurate the matches are:

```bash
python evaluate.py --output output/output.csv --truth src/data/ground-truth.csv
```

Returns overall accuracy of carbon rating matching.

---

## ğŸ“ Directory Structure

```
eco-match-challenge/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ run.py                 # Main match runner
â”‚   â”œâ”€â”€ preprocess.py          # Preprocessing + index builder
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation script
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ matching.py        # FAISS + fuzzy matching logic
â”‚   â”‚   â”œâ”€â”€ pipeline.py        # Pipeline orchestrator
â”‚   â”‚   â”œâ”€â”€ embedding.py       # Sentence-BERT embedder
â”‚   â”‚   â”œâ”€â”€ cleaning.py        # Regex-based cleaner
â”‚   â”‚   â”œâ”€â”€ io.py              # Parquet/NPY file utils
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ open-source-ingredients.csv
â”‚       â”œâ”€â”€ test.csv
â”‚       â””â”€â”€ ground-truth.csv
â”œâ”€â”€ data/processed/
â”‚   â”œâ”€â”€ product_index.parquet
â”‚   â”œâ”€â”€ product_embeddings.npy
â”‚   â””â”€â”€ faiss_index.bin
â””â”€â”€ output/
    â””â”€â”€ output.csv
```

---

## ğŸ“¦ Requirements

Install dependencies:

```bash
pip install -r requirements.txt
```

Required:
- `sentence-transformers`
- `faiss-cpu`
- `thefuzz[speedup]`
- `numpy`, `pandas`, `pyarrow`

---

## ğŸ§  Key Concepts

- **FAISS**: High-speed vector similarity search over embeddings
- **Sentence-BERT**: Converts names into semantic embeddings
- **Fuzzy Re-ranking**: Improves robustness to text noise (e.g., brand names, order)

---

## ğŸ” Example Input â†’ Output

### Input (test.csv):

```
product_name
"Ben & Jerry's Chocolate Fudge Ice Cream 500ml"
"Organic Whole Milk 2L"
```

### Output (output.csv):

```
input_product,matched_product,carbon_rating
"Ben & Jerry's...",Ice Cream,C
"Organic Whole Milk...",Whole Milk,B
```

---

## ğŸ“ˆ Performance Notes

- FAISS is pre-indexed â†’ no recomputation at runtime
- Pipeline prints timing stats per step
- Average match latency ~70â€“100ms on CPU

---

## ğŸ› ï¸ Enhancement Ideas

- Persist fuzzy scores + visualize match candidates
- Switch to GPU FAISS (`faiss-gpu`)
- Add REST API with FastAPI/Flask for real-time querying
- Plug in OCR layer from receipt image

---

## ğŸ‘¤ Author

Built as part of the EcoMatch challenge for sustainable computing.

---
